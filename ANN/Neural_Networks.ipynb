{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Author: Prabin"
      ],
      "metadata": {
        "id": "5fVr0htu6w8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introdcution to Neuron and Neural Networks:\n",
        "\n",
        "A biological neuron is a fundamental unit of processing and intelligence. a It is infact a specialized cell designed to transmit information to other nerve cells, muscle, or gland cells. The information carried out by the neurons is vital for any and every activites and cognition showcased by living beings.\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Bzm3bFV7o0BSH4AcwzYlkjrpIWFQCPEZ' width=\"900\" height=\"400\"/>\n",
        "<Figcaption>Figure 1: Biological and Artificial Neuron</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "\n",
        "#### Structure of a Neuron:\n",
        "\n",
        "1. Dendrites: Receives signals from other neurons.\n",
        "2. Soma: Acts like the CPU, processing inputs.\n",
        "3. Axon: Functions like a cable, sending the output signal to another neuron.\n",
        "4. Synapse:  Acts as the point of connection for signal transmission betweeen axon of the first neuron and dendrite of the second neuron.\n",
        "\n",
        "\n",
        "#### Role of Neurons in Living beings:\n",
        "1. Input reception is done at sense organs and the sensory information is carried from the dendrites of the first neuron (sensory neuron) and sent to the next neuron. The goal is to ultimately send the information to the CPU or the Brain.\n",
        "2. Processing is done at the Brain (which is believed to be also consist of various a complex network of neurons), and the output is sent to the motor neurons.\n",
        "3. The motor neurons receive the output of the central processing unit as an input and transmits the information to the organ (such as hand) to carry out the action.\n",
        "\n",
        "#### Functionalities of Neural Networks:\n",
        "1. Neurons are believed to be arranged hierarchically, each layer processing specific aspects.\n",
        "2. Neurons specialize and fire only when specific criteria (or activation threshold limits) are met.\n",
        "3. Processed information cascades through layers (or from one neuron to another in a `chain` or `web`) until a final output is produced by the final layer.\n",
        "4. The output processed by the final layer then guides the system (or an organism) to act a certain way.\n",
        "\n",
        "#### Inspiration for Artificial Neural Networks:\n",
        "\n",
        "The biological neuron inspired some scientists to design a computational model (or system) that could mirror the working mechanism of the biological neurons. A single artificial neuron is called a Perceptron. And these perceptrons together make up Neural Networks that we use today. The early inspiration followed the following chain of thought:\n",
        "\n",
        "1. A system could be made to handle a certain stimuli (or perception) and produce some output to guide the decision process. The whole system could be a black box when viewed from outside that receives some input, does some processing and produces some output.\n",
        "\n",
        "2. The system could be made in multiple layers, where the first layer could perform operations on the raw data, and the subsequent layers could refine the output or perform some higher level operations.\n",
        "\n",
        "3.  Each neuron could contribute to solving a part of the overall problem, mirroring the division of work in biological systems.\n",
        "\n",
        "4. Each neuron could get activated only when a activation criteria is met. Thus the output of a particular neuron gets passed to an activation function (that could be set by a human) and if the output of the function met a certain threshold limit, the neuron could get activated.\n",
        "\n",
        "4. Neural Networks could function like nested functions, that execute a certain task on some input data and then pass the output to outer function and so on, thereby achieving its objective.\n"
      ],
      "metadata": {
        "id": "m_LFUK9a23uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## History:\n",
        "\n",
        "An artificial neural network is a computation model inspired by the structure of neural networks in the brain. It consists of large number of basic computing dedvices (or neurons) connected to each other in a complex communication network. The idea that neurons can be made to learn and to solve problems was proposed in the mid-20th century. So let's dive into the short history of the development of neural networks.\n"
      ],
      "metadata": {
        "id": "BlPJiueQzZQ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Algorithms:\n"
      ],
      "metadata": {
        "id": "FWpDu9oJCpn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### McCulloch Pits (MP):\n",
        "\n",
        "In 1943, neurophysiologist Warren McCulloch and mathematician Walter Pitts wrote a paper on how neurons might work. In order to describe how neurons in the brain might work, they modeled a simple neural network using electrical circuits. This is the first ever mathematical model of a biological neuron.\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=11TuF6NNEzmGTWlZFJynXubDWqoGqKTq_' width=\"600\" height=\"400\"/>\n",
        "<Figcaption>Figure 2: McCullough Pits Neuron</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "As shown in the figure, the McCulloch Pits Neuron has 2 functions, `g` and `f`.\n",
        "  * The first function 'g' does the simple aggregation (or summation) of the inputs.\n",
        "  * The second function 'f' does the threshold with a `theta` parameter. And produces output if the aggregated value exceeds the threshold.\n",
        "\n",
        "The inputs (x) are either excitatory or inhibitory. The excitatory inputs enable the sum to exceed the threshold parameter thereby creating a larger chance of exciting the neuron. The inhibitory inputs on the other hand does the opposite. For example, in an AND Gate, the input '1' serves excitatory purpose while the input '0' serve inhibitory purpose.\n",
        "\n",
        "In MP neuron, inputs are all boolean and the output is also boolean. So the function learned by the neuron is also boolean. The functions are:\n",
        "\n",
        "1. g(x) = ∑ $x_i$   (where, i= 1 to n inputs)\n",
        "\n",
        "2. y = f(g(x))     \n",
        "  * y = 1 if g(x) >= θ\n",
        "  * y = 0 if g(x) < θ\n",
        "\n",
        "Thus the function 'g' does the simple summation of all the inputs, and the function 'y' applies a step function (if value > threshold, output 1 else 0) to the sum.\n",
        "\n",
        "\\\n",
        "\n",
        "#### MP Neuron for Boolean Functions:\n",
        "\n",
        "1. OR Gate:\n",
        "\n",
        "OR Gate is activated when either of the input is 1, and deactivated only when both the inputs are 1. [Refer to OR Gate Truth Table if necessary].\n",
        "\n",
        "The threshold parameter (θ) = 1, and thus the function becomes:\n",
        "\n",
        "g(x) : x1 + x2 = 1\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=11OAOBEJFZeqmmq4yjUER15NMwnIfx7tU' width=\"900\" height=\"400\"/>\n",
        "<Figcaption>Figure 3: OR Gate decision boundary</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "The decision boundary as seen in the figure is linear. Any points on and above the line activates the neuron (y = 1) otherwise, the neuron is not activated.\n",
        "\n",
        "For multiple inputs (say 3), the function could be:\n",
        "\n",
        "g(x): x1+x2+x3 = 1\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1YKlREbHLmD3H0cWgkWGKH17A5J1v0lTJ' width=\"900\" height=\"500\"/>\n",
        "<Figcaption>Figure 3: OR Gate decision boundary</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "Here the decision boundary is a plane, and the neuron is activated based on the location of the points in and above the plane.\n",
        "\n",
        "\n",
        "2. AND Gate:\n",
        "\n",
        "The AND Gate is activated only when all of the inputs are 1, and deactivated when either of the input is 0.\n",
        "\n",
        "The threshold parameter (θ = 2), and thus the function becomses:\n",
        "\n",
        "g(x): x1 + x2 = 2\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1rCNIKGEVicd-oBPjHEy2XxEm1gtnTwq5' width=\"900\" height=\"400\"/>\n",
        "<Figcaption>Figure 4: AND Gate decision boundary</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "The decision boundary as seen in the figure is also linear. Any points on and above the line activates the neuron.\n",
        "\n",
        "3. NOR Gate:\n",
        "\n",
        "The NOR Gate is outputs 1 if all the inputs are 0, if only either of the inputs is 1, the gate is not activated. Thus the inputs can be regarded as inhibitory.\n",
        "The threshold is: θ = 0.\n",
        "\n",
        "4. NOT Gate:\n",
        "The NOT Gate outputs just the opposite value of the input. That is the output is 1 when the input is 0, and the output is 0 when the input is 1.\n",
        "The input is also inhibitory. The threshold is: θ = 0.\n",
        "\n",
        "\\\n",
        "\n",
        "So can any boolean function be represented with MP neuron? Let's take a case of XOR Gate.\n",
        "\n",
        "XOR Gate is activated when the inputs have different values, and not activated if both the inputs have the same value of either 0 or 1.\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=17vEcoTfbQdCUuAzrG8hQOITIKFW69dti' width=\"900\" height=\"500\"/>\n",
        "<Figcaption>Figure 5: XOR Gate</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "From the figure, it's pretty evident that the decision boundary is not linear. Thus the XOR problem can't be solved with a single MP neuron. The same problem was highlighted by Minsky and Papert in their book \"Perceptrons\", where they showed the inability of single-layer perceptrons to solve XOR.\n",
        "\n",
        "#### Limitations:\n",
        "\n",
        "1. Allows only boolean inputs, and produces only boolean outputs. Real valued inputs are not supported.\n",
        "\n",
        "2. Couldn't even solve Boolean parameters when the decision boundary is non-linear. Example: XOR problem.\n",
        "\n",
        "2. The paramter 'θ' is set manually because we have the information that certain combination of inputs result in a certain output. What if we don't know that information and these are to be learned by assigning weights to the inputs?\n",
        "\n",
        "3. No trainable weights of the parameters. In case of logic gates, the rule is well defined in Truth Table, that certain inputs will result in certain outputs. So what if they are to be used in some scenarios some input had higher priorities than the others?\n",
        "\n",
        "\n",
        "McCulloch and Pits were well aware that a single threshold unit would not solve all their problems. In fact, their paper proves that such a unit can represent the basic Boolean functions AND , OR , and NOT and then goes on to argue that any desired functionality can be obtained by connecting large numbers of units into (possibly recurrent) networks of arbitrary depth. The problem was that nobody knew how to train such networks."
      ],
      "metadata": {
        "id": "pdqqGwmcGcTV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hebbian learning:\n",
        "\n",
        "The first Hebbian network was successfully implemented at MIT in 1954.\n",
        "\n",
        "\"Neurons that fire together, wire together\"- Donald Hebb\n",
        "\n",
        "This statement means that if neuron 'Q' fires immediately after a neuron 'P' was fired, the connections between these neurons will be stronger. For example, say the first neuron may fire when you think of the name 'James Hetfied', and the second neuron may fire when you think of 'Metallica'. Overtime, since you often think about James and Metallica together, the connection between the two neurons strengthens. If the pre-synaptic neuron corresponds to the input layer and the post-synaptic neuron corresponds to the output neuron then the synapse is the weight connecting the two neurons.\n",
        "\n",
        "Hebbian learning rule can be explained with the equation:\n",
        "\n",
        "Δw = ηxz\n",
        "\n",
        "where,\n",
        "\n",
        "Δw =  weight updation,\n",
        "\n",
        "η = learning rate,\n",
        "\n",
        "x = value of pre-synaptic neuron,\n",
        "\n",
        "z = value of post-synaptic neuron.\n",
        "\n",
        "Notice that there is no error involved. The Hebbian rule is not supervised, where there is no need to calculate any error (with the target label) and calculate the feedback.\n",
        "\n",
        "Also notice the introduction of 'learning rate', but is implicit."
      ],
      "metadata": {
        "id": "HJqZ5qG5R4bE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perceptron and Perceptron Learning ALgorithm (PLA):\n",
        "\n",
        "The Perceptron is an upgraded version of McCulloch Pits (MP) neuron. This was originally proposed by Frank Rosenblatt in 1943, later refined and analyzed by Minsky and Papert in 1969.\n",
        "\n",
        "Let's look at the following picture of a Perceptron implementing an AND gate and analyze the difference between the MP neuron and the Perceptron.\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=19avwDoppFgQTamwYGW3EihsQxuKHTsST' width=\"900\" height=\"400\"/>\n",
        "<Figcaption>Figure 6: Perceptron (AND Gate)</Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "From the picture, the most peculiar difference is the presence of weights (w1,w2,w3,....). Yes, the inputs in the perceptron can be trainable. That is the most important difference from the MP neuron. Since the inputs are weighted, this model is much more flexible.\n",
        "\n",
        "Another important point is that the input can be non-boolean values. Unlike the MP neuron, this model has a function that calculates the weighted sum of the (real as well as boolean valued) inputs and the binary output is provided based on the threshold value.\n",
        "\n",
        "Talking about similarity, MP Neuron Model as well as the Perceptron model work on linearly separable data.\n",
        "\n",
        "\\\n",
        "#### PLA:\n",
        "\n",
        "Lets take a look at how the threshold parameter `θ` will be included within the weighted inputs and the learning algorithm is defined.\n",
        "\n",
        "<image>\n",
        "<center>\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1LdWBTt_zLqEp3UQJ3RrBi_-nhwJznmb2' width=\"900\" height=\"350\"/>\n",
        "<Figcaption>Figure 7: Perceptron Learning Algorithm (PLA) </Figcaption>\n",
        "</center>\n",
        "</image>\n",
        "\n",
        "Just like in the MP neuron, the perceptron takes an input, aggregates it and returns 1 only if the aggregated sum is more than some threshold else returns 0. This time the only difference being in the aggregation process, where the aggregation function is the weighted sum (rather than just the sum). The output is 1 if it exceeds the threshold. That's why this neuron is also called Threshold Logic Umit (TLU) or Linear Threshold Unit (LTU). The thresholding is done with the step function to the weighted sum.\n",
        "\n",
        "\n",
        "One might even say that the step function is pretty \"harse\" for other application other than logic gates. What do I mean by that? So let's say the threshold is 0.5, the step function outputs 1 (positive class) for 0.5 value, and outputs 0 (negative class) for 0.499 value. So in a way this function treats 0.1 and 0.499 in the same category, but treats 0.499 and 0.5 in a separate category! So is this always useful? Obviously this may result in misclassification. So we use other smoother activation functions, other than step function. One such example is a sigmoid activation function.\n",
        "\n",
        "What a perceptron does technically is to compute the dot product of weight matrix and the input matrix then add the bias term. This is the output of the first function `g`. Then the value obtained is passed through a step-function `f` which produces the final output as either 0 (not activated) or 1 (neuron activated).\n",
        "\n",
        "So how do we actually make the perceptron learn? Earlier we saw that Hebbian learning is an unsupervised learning. The same concept of Hebbian learning can be extended for a supervised setting by considering the difference in obtained output and target output instead of just considering the obtained output. The Perceptron Learning Algorithm (PLA) is shown below:\n",
        "\n",
        "#### PLA Learning Rule:\n",
        "\n",
        "Since backpropagation wasn't developed yet, the perceptron's parameters were updated based on the Hebbian learning rule. The weight updation rule is:\n",
        "\n",
        "**$w_{i,j}$ = $w_{i,j}$ + η ($y_j$ - $ŷ_j$) $x_i$**\n",
        "\n",
        "* $w_{i,j}$ is the connection weight between the ith and jth neuron. (ith means input, and jth is the perceptron in this case)\n",
        "\n",
        "* $x_i$ is ith training input\n",
        "\n",
        "* $ŷ_j$ is the output of jth neuron (perceptron)\n",
        "\n",
        "* $y_j$ is the target output\n",
        "\n",
        "* η is the learning rate.\n",
        "\n",
        "\n",
        "It's not necessary to repeat the same experiments on logic gates like we did in MP neuron, because the results and insights are similar. The only difference is that there are weighted inputs to consider. There may be multiple solutions to the linear equations obtained. Like in MP neuron, Perceptron can't solve problems like XOR problem like highlighted in Minsky's book. Such problems couldn't be solved by Logistic regression classifiers as well but since researchers had expected a lot from Perceptrons, they were greatly disappointed and they dropped neural networks altogether to focus on logic, problem solving and search.\n",
        "\n",
        "Note that stacking perceptrons in layers (called as MultiLayer Perceptrons- MLP) can solve this problem.\n",
        "\n"
      ],
      "metadata": {
        "id": "KjC0yWvRGwd5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron Implementation from Scratch:"
      ],
      "metadata": {
        "id": "7uZi8RTxxr6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "GaqNxZwyxwVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets define Perceptron class with methods (functions) for fitting the Perceptron model and making predictions.\n",
        "\n",
        "* Parameters:\n",
        "  1. η = learning rate, set to about 0.01\n",
        "  2. n_iter = no of iterations/passes, lets set to about 10\n",
        "\n",
        "* Attributes:\n",
        "  1. w = 1d array for weights\n",
        "  2. errors = list for no of misclassification in each epoch\n",
        "\n",
        "\\\n",
        "\n",
        "* X is the training vector with shape = [n_samples, n_features]\n",
        "\n",
        "* y is the target label with shape = [n_samples]"
      ],
      "metadata": {
        "id": "LthaUqgezKhZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron(object):\n",
        "\n",
        "    def __init__(self, eta=0.01, n_iter=10):\n",
        "        self.eta = eta\n",
        "        self.n_iter = n_iter\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.w_ = np.zeros(1 + X.shape[1])        # Initializes the weight vector w_ with zeros. The size is 1 + X.shape[1] to include the bias term.\n",
        "        self.errors_ = []\n",
        "\n",
        "        for _ in range(self.n_iter):              # loop over the no of iterations\n",
        "            errors = 0\n",
        "            for xi, target in zip(X, y):\n",
        "                update = self.eta * (target - self.predict(xi))\n",
        "                self.w_[1:] += update * xi\n",
        "                self.w_[0] += update\n",
        "                errors += int(update != 0.0)\n",
        "            self.errors_.append(errors)\n",
        "        return self\n",
        "\n",
        "\n",
        "    def net_input(self, X):\n",
        "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
      ],
      "metadata": {
        "id": "kkQ3AuxlyvwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "X = np.array([[2, 3], [1, 1], [2, 1], [4, 5], [3, 4], [3, 2]])\n",
        "y = np.array([1, -1, -1, 1, 1, -1])"
      ],
      "metadata": {
        "id": "zWHjjGFQ1dN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "perceptron = Perceptron(eta=0.1, n_iter=10)"
      ],
      "metadata": {
        "id": "qz3Klm5X1sIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model\n",
        "perceptron.fit(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEkghIbr1wEi",
        "outputId": "3d278517-be05-41e6-bcd7-bfbad83ff080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Perceptron at 0x7ddc026996c0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the errors in each epoch\n",
        "print(\"Errors in each epoch:\", perceptron.errors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJymrE-q13si",
        "outputId": "200cb997-0211-45ca-b942-b95931c96296"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errors in each epoch: [3, 3, 1, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "print(\"Predictions:\", perceptron.predict(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHgv7w6P17K1",
        "outputId": "76023977-df0c-4048-c555-bcf592c1558b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [ 1 -1 -1  1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Material:\n",
        "\n",
        "MP: [https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1]\n",
        "\n",
        "NN: [https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/index.html]\n",
        "\n",
        "Perceptron: [https://towardsdatascience.com/perceptron-the-artificial-neuron-4d8c70d5cc8d]\n",
        "\n"
      ],
      "metadata": {
        "id": "kR5leEOfKdUc"
      }
    }
  ]
}
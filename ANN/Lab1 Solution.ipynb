{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DEu_NwcB2ijP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptron Learning Algorithm (PLA):\n",
        "\n",
        "PLA learns the parameters (weights) with the following formula:\n",
        "\n",
        "new_weights = old_weights + error * Input * learning rate."
      ],
      "metadata": {
        "id": "2N68YSQL-uLv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation(x):\n",
        "    return 1 if x > 0 else 0"
      ],
      "metadata": {
        "id": "WKAH7af82lCp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_perceptron(inputs, desire_outputs):\n",
        "  global w0,w1,b\n",
        "  w0 = random.uniform(-1,1)\n",
        "  w1 = random.uniform(-1,1)\n",
        "  b = random.uniform(-1,1)\n",
        "  epochs = 100\n",
        "  learning_rate = 0.1\n",
        "  for epoch in range(epochs):\n",
        "    total_error =0\n",
        "    for i in range(len(inputs)):\n",
        "      A, B = inputs[i]\n",
        "      target_output = desire_outputs[i]\n",
        "      predicted = activation(w0*A + w1*B +b)  # Forward pass\n",
        "      error = target_output - predicted       # Compute loss (error)\n",
        "\n",
        "      w0 += error * A * learning_rate         # Optimize the weights with the help of above loss.\n",
        "      w1 += error * B * learning_rate\n",
        "      b += error * learning_rate\n",
        "      total_error += abs(error)\n",
        "    if total_error == 0:\n",
        "      print(f\"Training completed in {epoch+1} epochs.\")\n",
        "      break\n",
        "  else:\n",
        "    print(\"Maximum epoch reached\")"
      ],
      "metadata": {
        "id": "e5uJqg4F2o58"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_perceptron(input):\n",
        "  A,B = input\n",
        "  predicted = activation(w0 * A + w1 * B +b)\n",
        "  return predicted"
      ],
      "metadata": {
        "id": "Hjx_0zmA33Vz"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. AND Gate:\n",
        "\n",
        "Lets train the above perceptron to learn the AND gate. We do so using the following training data. i.e. Inputs and desired outputs for AND gate."
      ],
      "metadata": {
        "id": "xj5n6E7M-Ujr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AND_inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "AND_desired_outputs = [0, 0, 0, 1]"
      ],
      "metadata": {
        "id": "X5tbtxbM3upo"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the perceptron on AND data"
      ],
      "metadata": {
        "id": "UM7LWSrx_OLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_perceptron(AND_inputs, AND_desired_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6p9lAV432iu",
        "outputId": "787a0257-0e8a-4902-e3dc-87e7c2860f25"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 4 epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the perceptron trained on AND data"
      ],
      "metadata": {
        "id": "alGQIRqP_UqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_data in AND_inputs:\n",
        "    print(f\"Input: {input_data}, Predicted Output: {test_perceptron(input_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdkYJwfa4s5E",
        "outputId": "267bf2ba-8f7b-49c3-8497-55c86d9afbc1"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (0, 0), Predicted Output: 0\n",
            "Input: (0, 1), Predicted Output: 0\n",
            "Input: (1, 0), Predicted Output: 0\n",
            "Input: (1, 1), Predicted Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. OR Gate:\n",
        "\n",
        "Lets train the same perceptron again to learn the OR gate. We must do so by creating a different training data for OR gate, which are given below:"
      ],
      "metadata": {
        "id": "wqpbBXfi_gR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OR_inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "OR_desired_outputs = [0, 1, 1, 1]"
      ],
      "metadata": {
        "id": "geMDLAdc-Nwy"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the perceptron on OR data:"
      ],
      "metadata": {
        "id": "gxmVTMHCAAfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_perceptron(OR_inputs, OR_desired_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCy3xdkM_-v_",
        "outputId": "b880d86e-0ac6-42ae-9e5f-30578b714a37"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed in 2 epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the perceptron on OR data:"
      ],
      "metadata": {
        "id": "C2eKbl-aAIzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_data in OR_inputs:\n",
        "    print(f\"Input: {input_data}, Predicted Output: {test_perceptron(input_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuh3DSt7AHA1",
        "outputId": "80e58d03-0b76-4b7f-af42-ab298c371b03"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (0, 0), Predicted Output: 0\n",
            "Input: (0, 1), Predicted Output: 1\n",
            "Input: (1, 0), Predicted Output: 1\n",
            "Input: (1, 1), Predicted Output: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. XOR Gate:\n"
      ],
      "metadata": {
        "id": "UEsGM0bpARzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XOR_inputs = [(0,0), (0,1), (1,0), (1,1)]\n",
        "XOR_desired_outputs = [0, 1, 1, 0]"
      ],
      "metadata": {
        "id": "3HTDZF8kAQcA"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_perceptron(XOR_inputs, XOR_desired_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39kRZzP8AXtc",
        "outputId": "2cabce17-2842-4af9-ddd0-8fad876032ba"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum epoch reached\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_data in OR_inputs:\n",
        "    print(f\"Input: {input_data}, Predicted Output: {test_perceptron(input_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFYn8Py6AbH0",
        "outputId": "03b06b77-6400-46a3-c117-3ecd29cf329c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: (0, 0), Predicted Output: 1\n",
            "Input: (0, 1), Predicted Output: 1\n",
            "Input: (1, 0), Predicted Output: 0\n",
            "Input: (1, 1), Predicted Output: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to note that XOR problem is not linearly separable(i.e. **the decision boundary is not a straight line**), thus a single neuron can't learn such function.\n",
        "\n",
        "Thus the above outputs shows the incorrect implementation of XOR gate.\n",
        "\n",
        "The solution is to use MLP (multi-layer perceptron) which can learn any sorts of complex decision boundary. They are also called `Universal Approximators`."
      ],
      "metadata": {
        "id": "Yig2imqHAdqv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1qyQkbWBAclo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}